---
title: "Project2"
author: "Anna"
date: "11/20/2020"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

## Introduction

The dataset I chose to analyze is from the UCI Machine Learning Repository, and is called the Contraceptive Method Choice Data Set. It is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The variables in this survey are the age, education, religion, and work status of the wife, the education of the husband, the number of children ever born, the standard of living, the level of media exposure the couple has, and the method of contraception. The original dataset includes three methods of contraception: no use, short-term use, and long-term use. For the purposes of this project, however, I will be combining short-term use and long-term use to create a binary variable. Age and number of children are numerical variables in this dataset; the other variables are categorical. There are 1473 observations in this dataset.

```{r}
cmc <- read_csv("cmc.csv")
cmc <- cmc %>% mutate(contraceptive=NULL)
head(cmc)
```

## MANOVA Testing

Before testing this data using MANOVA, I first had to test the assumptions of multivariate normality and homogeneity of covariance matrices. I chose to test the effect of the number of children and the age of the wife on the method of contraception. 

```{r}
library(rstatix)

group <- cmc$bin_contraceptive 
DVs <- cmc %>% select(wife_age,num_children)

#Test multivariate normality for each group (null: assumption met)
sapply(split(DVs,group), mshapiro_test)

#If any p<.05, stop. If not, test homogeneity of covariance matrices

#Box's M test (null: assumption met)
box_m(DVs, group)
#View covariance matrices for each group
lapply(split(DVs,group), cov)

```
I found that both of the assumptions of multivariate normality and homogeneity of covariance matrices were violated. This will serve as a limitation for my final results, but I will still conduct the MANOVA test. 

```{r}
man1<-manova(cbind(wife_age,num_children)~bin_contraceptive, data=cmc)
summary(man1)
```
The MANOVA test is significant! I will follow up with univariate ANOVAs for each variable. 

```{r}
summary.aov(man1)
```
Both are significant, meaning that for both the wife's age and the number of children, the method of contraception differs significantly within the group. Since the method of contraception is a binary variable, I do not need to do post-hoc testing on this data. I have done a total of three tests, meaning I must change the alpha to 0.0167. Even so, all of my test results are still significant. 

## Randomization Testing

I next conducted a randomization test on the mean difference between the number of children of women in each of the two contraceptive groups. The null hypothesis is that the mean difference in number of children does not differ significantly from the null distribution generated by the randomization test, and the alternative is that it does differ significantly. 
```{r}
cmc%>%group_by(bin_contraceptive)%>%summarize(means=mean(num_children))%>%summarize(`mean_diff`=diff(means))

rand_dist<-vector()
for(i in 1:5000){
  new<-data.frame(num_children=sample(cmc$num_children),bin_contraceptive=cmc$bin_contraceptive)
  rand_dist[i]<-mean(new[new$bin_contraceptive==1,]$num_children)-mean(new[new$bin_contraceptive==0,]$num_children)}

{hist(rand_dist,main="",ylab="", xlab="Null Distribution"); abline(v = c(-0.5699222, 0.5699222),col="red")}
mean(rand_dist>0.5699222 | rand_dist < -0.5699222)
```
The p-value for this test is 0, meaning that the probability of obtaining a result as extreme as the observed difference of means from within the null distribution created through the randomization test is zero. The above plot of the null distribution illustrates this zero result; one of the boundaries, which signifies the test statistics, is not even visible since it is so far to the right. Therefore, I reject the null hypothesis and conclude that the mean difference in the number of children between the two contraceptive groups differs significantly. This means that women who use birth control have a significantly different number of children than those who do not. 

## Linear Regression

I next did a linear regression to determine if the age of the wife and the family's media exposure impacted the number of children born. I started by creating a new character-based "media" variable so that it would work properly in ggplot, and I centered the ages of the wives. I then ran the regression.

```{r}
cmc <- cmc %>%mutate(bin_media=ifelse(media==1,"not good","good"))
cmc$wife_age_c <- cmc$wife_age - mean(cmc$wife_age)
fit<-lm(num_children~media*wife_age_c, data= cmc)
summary(fit)
```
This revealed that controlling for age, media exposure has a positive effect on the number of children, increasing the number by 0.69 children when it is good. When controlling for media exposure, the age of the wife also has a positive effect, increasing the number of children by 0.15 per year of the wife's age. The interaction between these two variables has a negative effect. I have plotted these results below. 

```{r}
ggplot(cmc, aes(wife_age_c,num_children, color=bin_media))+geom_point()+geom_smooth(method="lm", se=F) +xlab("Age of Wife (centered)") + ylab("Number of Children")
```
I then checked the assumptions of this regression. 
```{r}
lin_samp <- sample_n(cmc, 25)
ggplot(lin_samp, aes(wife_age_c, num_children)) + geom_point() + geom_smooth(method="lm", se=F)
resids<-fit$residuals
fitvals<-fit$fitted.values
data.frame(resids,fitvals)%>%ggplot(aes(fitvals,resids))+geom_point()+geom_hline(yintercept=0)
par(mfrow=c(1,2)); hist(resids)
```
From these tests, it appears that homoskedasticity is violated, as the points fan out. The residuals are more or less normally distributed, and the data is linear.

Because the data is not homoskedastic, I recomputed the regression results using robust standard errors. 
```{r}
library(sandwich); library(lmtest)
coeftest(fit, vcov = vcovHC(fit))
```
This does not change any of the intercepts, but slightly changes the significance of each. In the initial regression, only media exposure and the wife's age were significant; the interaction between these variables was not. This is true of the recomputed regression results, but the specific p-values are different. Overall, the robust standard errors do not change the results. Overall, this model explains 0.2957 (29.57%) of the variation in the number of children a woman has. 

## Bootstrapped Standard Errors

I then reran the above regression using bootstrapped standard errors, which I found by resampling observations. 

```{r}
samp_distn<-replicate(5000, {
  boot_dat <- sample_frac(cmc, replace=T) #take bootstrap sample of rows  
  fit <- lm(num_children~media*wife_age_c, data=boot_dat) #fit model on bootstrap sample  
  coef(fit) #save coefs
  }) 
samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)
```
These standard errors differ only very slightly from the robust standard errors calculated above, but differ a bit more  from the original standard errors. The standard error for media is slightly lower, while the standard errors for the wife's ages and the interaction term are slightly higher. 

## Logistic Regression

I chose to run a logistic regression to try to predict whether or not a woman uses contraception from the number of children she has and the media exposure she has.  

```{r}
fit2<-glm(bin_contraceptive~num_children+bin_media, family="binomial", data=cmc)
coeftest(fit2)
```
Controlling for media exposure, for every additional child born, odds of using contraception change by a factor of 1.14. Controlling for number of children, odds of using contraception for women who do not have good media exposure change by a factor of 0.26. 
```{r}
cmc$prob <- predict(fit2,type="response")
cmc$predicted <- ifelse(cmc$prob>.5,"1","0")
table(truth=cmc$bin_contraceptive, prediction=cmc$predicted)%>%addmargins
```
```{r}
class_diag <- function(probs,truth){
  #CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  f1=2*(sens*ppv)/(sens+ppv)

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,f1,auc)
}
class_diag(cmc$prob, cmc$bin_contraceptive)

cmc <- cmc %>%mutate(ggbin_contraceptive=ifelse(bin_contraceptive==1,"yes","no"))
cmc$logit<-predict(fit2)
ggplot(cmc, aes(logit, fill=ggbin_contraceptive))+geom_density(alpha=.3)

library(plotROC)  
ROCplot<-ggplot(cmc)+geom_roc(aes(d=bin_contraceptive,m=prob), n.cuts=0) 
ROCplot
calc_auc(ROCplot)
```
Both accuracy and precision are relatively low, which is not a good sign for this model. Sensitivity is very high and specificity is very low; this indicates that while most contraception users were correctly classified, few people who do not use contraception were correctly classified. The AUC is 0.65, which can be interpreted as poor performance. The plot of the log-odds shows that the two groups are largely overlapping, which is not good, and the ROC curve is strongly curved, leading to the poor AUC value. 

## Logistic Regression (on everything now!)

Finally, I ran a logistic regression using every variable to predict whether or not a woman uses contraception. 

```{r}
fit3<-glm(bin_contraceptive~num_children+bin_media+wife_age_c+wife_ed+husband_ed+wife_religion+wife_work+standard, family="binomial", data=cmc)
coeftest(fit3)
cmc$prob2 <- predict(fit3,type="response")
class_diag(cmc$prob2, cmc$bin_contraceptive)
```
In this case, the regression appears to have performed somewhat better than when I only predicted using two variables. The AUC is now 0.71, which is classified as fair. The accuracy are precision are still low, although slightly higher than before. The sensitivity has decreased and the specificity has increased, meaning that although the model still labels people who use contraception better than those who do not, the difference has decreased.  

Next, I used cross-validation to test how the model fits to unknown data. 
```{r}
set.seed(456)
k=10 

data<-cmc[sample(nrow(cmc)),] #randomly order rows
folds<-cut(seq(1:nrow(cmc)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  
  truth<-test$bin_contraceptive ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)
  fit<-glm(bin_contraceptive~num_children+bin_media+wife_age_c+wife_ed+husband_ed+wife_religion+wife_work+standard,
           family="binomial",data=train)
  
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
```
The AUC rounds 0.71, which is the same as it was with the in-sample metrics. Similarly,  all other metrics differ slightly, but only past the thousandths place, and would round to approximately the same value. This indicates that it is likely not overfitting too much. 

To test this theory, I ran LASSO, which will find the most predictive variables. 

```{r}
library(glmnet)
y<-as.matrix(cmc$bin_contraceptive) 
x<-model.matrix(bin_contraceptive~bin_media+num_children+wife_age_c+wife_ed+husband_ed+wife_religion+wife_work+standard, data=cmc)[,-1] 
x<-scale(x) 

cv <- cv.glmnet(x,y, family="binomial")

{plot(cv$glmnet.fit, "lambda", label=TRUE); abline(v = log(cv$lambda.1se)); abline(v = log(cv$lambda.min),lty=2)}

cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)
```
Out of all eight of my predictor variables, only the husband's education level and the wife's work status were not predictive. I retained all six of the other variables. 

Finally, I performed another cross-validation to see how the model works when these two variables are not used. 
```{r}
set.seed(456)
k=10 

data<-cmc[sample(nrow(cmc)),] #randomly order rows
folds<-cut(seq(1:nrow(cmc)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  
  truth<-test$bin_contraceptive ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)
  fit<-glm(bin_contraceptive~num_children+bin_media+wife_age_c+wife_ed+wife_religion+standard,family="binomial",data=train)
  
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
```
The AUC is now 0.71, which is the same as it was previously. Similarly, all other values are approximately the same. This indicates that the model was not overfitting, even with the two non-predictive variables. 
